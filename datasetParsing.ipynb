{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21 Mil WikiDPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wiki_dpr\", \"psgs_w100.multiset.no_index\", streaming= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# print(next(iter(dataset['train'])))\n",
    "# embeds = np.array([doc['emb'] for doc in dataset['train']])\n",
    "# print(\"Embeddings:\")\n",
    "# print(embeds)\n",
    "\n",
    "# get the document of id 500\n",
    "# temp = dataset['train'].filter(lambda x: x['id'] == 0)\n",
    "# print(\"Document of id 500:\")\n",
    "# print(next(iter(temp)))\n",
    "\n",
    "# dataset_head = dataset['train'].skip(10000)\n",
    "# dataset_head = dataset_head.take(1)\n",
    "# print(\"Head:\")\n",
    "# print(next(iter(dataset_head)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohere wiki 485k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from datasets import load_dataset\n",
    "\n",
    "docs = load_dataset(f\"Cohere/wikipedia-22-12-simple-embeddings\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "\n",
    "    with open(\"embeds2.txt\", \"a\") as f:\n",
    "        f.write(str(doc['emb']))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Using Faiss KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset from the pre-downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset from the files in /batches/dataset in one big numpy array\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf, linewidth=np.inf)  # ensure numpy array won't truncate\n",
    "\n",
    "embeds = []\n",
    "for i in range(5):\n",
    "   \n",
    "    with open(f\"./Data/dataset/{i}-embeds-batch.txt\", \"r\") as f:\n",
    "        lines = [np.fromstring(line, sep=\" \") for line in f]\n",
    "        # normalize the embeddings\n",
    "        lines = [line / np.linalg.norm(line) for line in lines]\n",
    "        embeds.append(lines)\n",
    "        f.close()\n",
    "    # # write the normalized embeddings to a file line by line\n",
    "    # with open(f\"./Data/normalizedDataset/{i}-embeds-batch.txt\", \"a\") as f:\n",
    "    #     for line in lines:\n",
    "    #         line = np.array(line)\n",
    "    #         f.write(str(line).replace('[', '').replace(']', '').replace('  ',' '))\n",
    "    #         f.write('\\n')\n",
    "    #     f.close()\n",
    "    \n",
    "\n",
    "# Concatenate all lists into a\n",
    "embeds = np.concatenate(embeds, axis=0)\n",
    "\n",
    "# print the norm of the first 100 embeddings\n",
    "for i in range(10):\n",
    "    print(np.linalg.norm(embeds[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the clusters using Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use faiss to cluster the embeddings\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "ncentroids = 100\n",
    "niter = 300\n",
    "verbose = True\n",
    "gpu = True\n",
    "dim = 768\n",
    "kmeans = faiss.Kmeans(dim, ncentroids, niter = niter, verbose = verbose, gpu = gpu, \n",
    "                      min_points_per_centroid = 100, max_points_per_centroid = 100000, \n",
    "                      nredo = 2, spherical = True)\n",
    "\n",
    "kmeans.train(embeds.astype(np.float32))\n",
    "\n",
    "faiss.write_index(kmeans.index, \"./Data/KMeansFAISS/index.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping each point to its nearest centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "np.set_printoptions(threshold=np.inf, linewidth=np.inf)  # ensure numpy array won't truncate\n",
    "\n",
    "\n",
    "# read the faiss index\n",
    "index = faiss.read_index(\"./Data/KMeansFAISS/index.bin\")\n",
    "\n",
    "# mapping each point to its nearest centroid\n",
    "_, I = index.search(embeds, 2)\n",
    "\n",
    "# flatten the array of arrays\n",
    "I_flat = [item for sublist in I for item in sublist]\n",
    "\n",
    "# write the labels to a file where I_flat is the centroid index for each embedding\n",
    "with open(\"./Data/labels/faiss_labels.txt\", \"a\") as f:\n",
    "    # remove the string array representation of the list\n",
    "    f.write(str(I_flat).replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\").replace(\"  \", \" \").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the centroids to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each centroid file, we need to manually remove square brackets \"[\" or \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf, linewidth=np.inf)  # ensure numpy array won't truncate\n",
    "centroids_filename = \"D:/Boody/GP/Indexer/RAGn-Roll-Indexer/Data/centroids/final-centroids.txt\"\n",
    "with open(centroids_filename, \"a\") as f:\n",
    "    # print the centroids line by line\n",
    "    # make the printing so that it execludes any squared brackets or double spacing also remove that first space \n",
    "    # also remove all the commas\n",
    "    # also remove all the white spaces at the very beginig and end of each line\n",
    "\n",
    "    for i in range(ncentroids):\n",
    "        f.write(str(kmeans.centroids[i]).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"  \", \" \").strip())\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the labels file, where each item from dataset (in batches) is mapped to its closest two centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the nearest clusters to the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read queries and compute nearest cloisters\n",
    "import faiss \n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf, linewidth=np.inf)  # ensure numpy array won't truncate\n",
    "\n",
    "# read the faiss index\n",
    "index = faiss.read_index(\"./Data/KMeansFAISS/index.bin\")\n",
    "\n",
    "with open(\"./Data/query.txt\", \"r\") as f:\n",
    "    lines = [np.fromstring(line, sep=\" \") for line in f]\n",
    "    queries = np.array(lines)\n",
    "\n",
    "#Normalize the query\n",
    "queries = queries / np.linalg.norm(queries)\n",
    "\n",
    "_, I = index.search(queries, 10)\n",
    "I_flat = [i for i in I]\n",
    "\n",
    "# write the labels to a file where I_flat is the centroid index for each embedding\n",
    "with open(\"./Data/labels/query_faiss_labels.txt\", \"a\") as f:\n",
    "    f.write(str(I_flat).replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\").replace(\"  \", \" \").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each label file, we need to manually remove double spaces \"  \" and any square brackets \"[\" or \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_labels(X, centroids):\n",
    "    # // make this of dimension x.shape[0][num]\n",
    "    assigned_custers = 2\n",
    "    labels = np.empty(X.shape[0]*assigned_custers, dtype=np.int64)\n",
    "    for i, x in enumerate(X):\n",
    "        # L2\n",
    "        # distances = np.linalg.norm(x - centroids, axis=1)\n",
    "\n",
    "        # cosine similarity\n",
    "        # distances = np.dot(x, centroids.T) / (np.linalg.norm(x) * np.linalg.norm(centroids))\n",
    "\n",
    "        # use inner product to calculate the distances\n",
    "        distances = np.inner(centroids, x)\n",
    "\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        min_index = sorted_indices[0]\n",
    "        second_min_index = sorted_indices[1]\n",
    "        labels[2 * i] = min_index\n",
    "        labels[2*i + 1] = second_min_index\n",
    "    return labels\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf, linewidth=np.inf)  # ensure numpy array won't truncate\n",
    "centroids_filename = \"D:/Boody/GP/Indexer/RAGn-Roll-Indexer/Data/centroids/final-centroids.txt\"\n",
    "\n",
    "# #print the norm of the centroids\n",
    "# centroids = np.loadtxt(centroids_filename)\n",
    "# for i in range(100):\n",
    "#     print(np.linalg.norm(centroids[i]))\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    with open(f\"D:/Boody/GP/Indexer/RAGn-Roll-Indexer/Data/dataset/{i}-embeds-batch.txt\", \"r\") as f:\n",
    "        # read each line and convert to numpy array\n",
    "        lines = [np.fromstring(line, sep=\" \") for line in f]\n",
    "        # convert to numpy array\n",
    "        embeds = np.array(lines)\n",
    "        # print(embeds.shape)\n",
    "        centroids = np.loadtxt(centroids_filename)\n",
    "        # #normalize the centroids\n",
    "        # centroids = centroids / np.linalg.norm(centroids, axis=1)[:, np.newaxis]\n",
    "        # #normalize the embeddings\n",
    "        # embeds = embeds / np.linalg.norm(embeds, axis=1)[:, np.newaxis]\n",
    "        labels = assign_labels(embeds, centroids)\n",
    "        with open(f\"D:/Boody/GP/Indexer/RAGn-Roll-Indexer/Data/labels/labels{i}.txt\", \"a\") as f:\n",
    "            f.write(str(labels).replace('[', '').replace(']', '').replace('  ', ' ').replace('  ', ' ').strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KD tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing KD tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "from scipy.spatial import cKDTree\n",
    "tree = KDTree(embeds) # leafSize = 16\n",
    "# tree = cKDTree(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_data, index, tree_nodes, node_bounds = tree.get_arrays()\n",
    "tree_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running K means on each batch\n",
    "- 100k batches\n",
    "- each batch 100 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop on folders\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, linewidth=np.inf)  # ensure numpy array won't truncate\n",
    "\n",
    "with open(\"./cmake-build-debug/batches/1-embeds-batch.txt\", \"r\") as f:\n",
    "    # read each line and convert to numpy array\n",
    "    lines = [np.fromstring(line, sep=\" \") for line in f]\n",
    "    # convert to numpy array\n",
    "    embeds = np.array(lines)\n",
    "    # apply kmeans on embeds and get the centroids\n",
    "    kmeans = KMeans(n_clusters = 100).fit(embeds)\n",
    "    with open(\"./cmake-build-debug/batches/centroids1.txt\", \"a\") as f:\n",
    "        f.write(str(kmeans.cluster_centers_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting all centroid files into one array\n",
    "* Make sure that centroids files don't have any squared brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = np.loadtxt(\"./cmake-build-debug/batches/centroids0.txt\")\n",
    "centroids = np.concatenate((centroids, np.loadtxt(\"./cmake-build-debug/batches/centroids1.txt\")), axis=0)\n",
    "centroids = np.concatenate((centroids, np.loadtxt(\"./cmake-build-debug/batches/centroids2.txt\")), axis=0)\n",
    "centroids = np.concatenate((centroids, np.loadtxt(\"./cmake-build-debug/batches/centroids3.txt\")), axis=0)\n",
    "centroids = np.concatenate((centroids, np.loadtxt(\"./cmake-build-debug/batches/centroids4.txt\")), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying K-means on all the centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply kmeans on embeds and get the centroids\n",
    "kmeans = KMeans(n_clusters = 100).fit(centroids)\n",
    "with open(\"./cmake-build-debug/batches/final-centroids.txt\", \"a\") as f:\n",
    "    f.write(str(kmeans.cluster_centers_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning labels to each batch according to the final centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_labels(X, centroids):\n",
    "    # // make this of dimension x.shape[0][num]\n",
    "    assigned_custers = 2\n",
    "    labels = np.empty(X.shape[0]*assigned_custers, dtype=np.int64)\n",
    "    for i, x in enumerate(X):\n",
    "        distances = np.linalg.norm(x - centroids, axis=1)\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        min_index = sorted_indices[0]\n",
    "        second_min_index = sorted_indices[1]\n",
    "        labels[2 * i] = min_index\n",
    "        labels[2*i + 1] = second_min_index\n",
    "    return labels\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf, linewidth=np.inf)  # ensure numpy array won't truncate\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    with open(f\"./cmake-build-debug/batches/dataset/{i}-embeds-batch.txt\", \"r\") as f:\n",
    "        # read each line and convert to numpy array\n",
    "        lines = [np.fromstring(line, sep=\" \") for line in f]\n",
    "        # convert to numpy array\n",
    "        embeds = np.array(lines)\n",
    "        # print(embeds.shape)\n",
    "        centroids = np.loadtxt(\"./cmake-build-debug/batches/centroids/final-centroids.txt\")\n",
    "        labels = assign_labels(embeds, centroids)\n",
    "        with open(f\"./cmake-build-debug/batches/labels/labels{i}.txt\", \"a\") as f:\n",
    "            f.write(str(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the embeds file\n",
    "with open(\"./cmake-build-debug/batches/dataset/1-embeds-batch.txt\", \"r\") as f:\n",
    "    lines = [np.fromstring(line, sep=\" \") for line in f]\n",
    "    embeds = np.array(lines)\n",
    "\n",
    "# read the labels and convert it to a list\n",
    "with open(\"./cmake-build-debug/batches/labels/labels1.txt\", \"r\") as f:\n",
    "    labels =[int(x) for x in next(f).split()]   \n",
    "\n",
    "# loop on the labels \n",
    "for i in range(100):\n",
    "    indices = [j//2 for j, x in enumerate(labels) if x == i]\n",
    "    current_embeds = embeds[indices]\n",
    "    with open(f\"./cmake-build-debug/batches/clusters/{i}.txt\", \"a\") as f:\n",
    "        f.write(str(current_embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    with open(f\"./cmake-build-debug/batches/clusters/cluster{i}.txt\", \"a\") as f:\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering using Faiss IVF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset from the pre-downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset from the files in /batches/dataset in one big numpy array\n",
    "import numpy as np\n",
    "\n",
    "embeds = []\n",
    "for i in range(5):\n",
    "    with open(f\"D:/Boody/GP/Indexer/RAGn-Roll-Indexer/Data/dataset/{i}-embeds-batch.txt\", \"r\") as f:\n",
    "        lines = [np.fromstring(line, sep=\" \") for line in f]\n",
    "        # normalize the embeddings\n",
    "        # lines = [line / np.linalg.norm(line) for line in lines]\n",
    "        embeds.append(np.array(lines))\n",
    "\n",
    "# Concatenate all lists into a\n",
    "embeds = np.concatenate(embeds, axis=0)\n",
    "\n",
    "# print the norm of the first 100 embeddings\n",
    "for i in range(100):\n",
    "    print(np.linalg.norm(embeds[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the clusters using Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use faiss to cluster the embeddings\n",
    "import faiss\n",
    "\n",
    "# create an inverted index\n",
    "nlist = 256\n",
    "m = 1\n",
    "k = 5\n",
    "d = 768\n",
    "coarse_quantizer = faiss.IndexFlatL2(d)\n",
    "index = faiss.IndexIVFPQ(coarse_quantizer, d, nlist, m, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.train(embeds)\n",
    "index.add(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is a pre-transform, you can also use\n",
    "# invlists = faiss.extract_index_ivf(index).invlists\n",
    "invlists = index.invlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_invlist(invlists, l):\n",
    "    \"\"\" returns the inverted lists content as a pair of (list_ids, list_codes).\n",
    "    The codes are reshaped to a proper size\n",
    "    \"\"\"\n",
    "    invlists = faiss.downcast_InvertedLists(invlists)\n",
    "    ls = invlists.list_size(l)\n",
    "    list_ids = np.zeros(ls, dtype='int64')\n",
    "    ids = codes = None\n",
    "    try:\n",
    "        ids = invlists.get_ids(l)\n",
    "        if ls > 0:\n",
    "            faiss.memcpy(faiss.swig_ptr(list_ids), ids, list_ids.nbytes)\n",
    "        codes = invlists.get_codes(l)\n",
    "        if invlists.code_size != faiss.InvertedLists.INVALID_CODE_SIZE:\n",
    "            list_codes = np.zeros((ls, invlists.code_size), dtype='uint8')\n",
    "        else:\n",
    "            # it's a BlockInvertedLists\n",
    "            npb = invlists.n_per_block\n",
    "            bs = invlists.block_size\n",
    "            ls_round = (ls + npb - 1) // npb\n",
    "            list_codes = np.zeros((ls_round, bs // npb, npb), dtype='uint8')\n",
    "        if ls > 0:\n",
    "            faiss.memcpy(faiss.swig_ptr(list_codes), codes, list_codes.nbytes)\n",
    "    finally:\n",
    "        if ids is not None:\n",
    "            invlists.release_ids(l, ids)\n",
    "        if codes is not None:\n",
    "            invlists.release_codes(l, codes)\n",
    "    return list_ids, list_codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get content of inverted list #123\n",
    "list_ids, list_codes = get_invlist(invlists, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf, linewidth=np.inf)  # ensure numpy array won't truncate\n",
    "\n",
    "# use the get_invlist function to get the list_ids and save them the the labels files each in a file indexed from 0 to 255\n",
    "for i in range(256):\n",
    "    list_ids, list_codes = get_invlist(invlists, i)\n",
    "    with open(f\"D:/Boody/GP/Indexer/RAGn-Roll-Indexer/Data/labels/labels{i}.txt\", \"a\") as f:\n",
    "        # make the printing so that it execludes any squared brackets or double spacing also remove that first space \n",
    "        # also remove all the white spaces at the very beginig and end of the string\n",
    "        f.write(str(list_ids).replace('[', '').replace(']', '').replace('  ', ' ').replace('  ', ' ').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use list_ids to map each vector from embeds to its corresponding label using the indices in list_ids and output the clusters to the clusters folder\n",
    "for i in range(256):\n",
    "    with open(f\"D:/Boody/GP/Indexer/RAGn-Roll-Indexer/Data/clusters/{i}.txt\", \"a\") as f:\n",
    "\n",
    "        with open(f\"D:/Boody/GP/Indexer/RAGn-Roll-Indexer/Data/labels/labels{i}.txt\", \"r\") as f2:\n",
    "            labels =[int(x) for x in next(f2).split()]   \n",
    "\n",
    "        for j in range(len(labels)):\n",
    "            # make the printing so that it execludes any squared brackets or double spacing also remove that first space \n",
    "            # also remove all the white spaces at the very beginig and end of the string\n",
    "            f.write(str(embeds[labels[j]]).replace('[', '').replace(']', '').replace('  ', ' ').replace('  ', ' ').strip())\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the clusters from the folder file by file and calculate the medoid for each cluster file and output the medoids to one file in centroids folder\n",
    "import numpy as np\n",
    "\n",
    "for i in range(256):\n",
    "    with open(f\"D:/Boody/GP/Indexer/RAGn-Roll-Indexer/Data/clusters/{i}.txt\", \"r\") as f:\n",
    "        lines = [np.fromstring(line, sep=\" \") for line in f]\n",
    "        embeds = np.array(lines)\n",
    "        medoid = embeds[np.argmin(np.sum(np.abs(embeds[:, np.newaxis] - embeds), axis=2), 0)]\n",
    "        with open(f\"D:/Boody/GP/Indexer/RAGn-Roll-Indexer/Data/centroids/test-centroids.txt\", \"a\") as f:\n",
    "            f.write(str(medoid).replace('[', '').replace(']', '').replace('  ', ' ').replace('  ', ' ').strip())\n",
    "            f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
